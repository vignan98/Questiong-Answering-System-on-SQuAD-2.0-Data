# -*- coding: utf-8 -*-
"""ROBERTA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17jL6qCtUMbZNCVheqAawm-fOUbMjWJ4b

# **IE7500 NLP Final Project**
# *SQuADv2.0 Answer Prediction Model using Fine tuning BERT models*

---

# **Importing Libraries**

Installing the transformers
"""

!pip install transformers

"""Installing other necessary libraries"""

import json
import pandas as pd
import numpy as np
import random
from transformers import AutoTokenizer
import os, json

"""# **Question Answering using BERT Model**

Reading the input from JSON file and storing in list
"""

def read_input(doc: str) -> tuple:    
    path = os.path.join(os.getcwd(), doc)
    with open(path, "rb") as json_file:
        input_dictionary = json.load(json_file)
    contexts= list() 
    questions=list() 
    answers =list()
    for i in input_dictionary['data']:
        for j in i['paragraphs']:
            k = j['context']
            for qa in j['qas']:
                q = qa['question']
                access = "plausible_answers" if "plausible_answers" in qa.keys() else 'answers'
                for a in qa[access]:
                    contexts.append(k)
                    questions.append(q)
                    answers.append(a)
    
    return contexts, questions, answers

"""Reading test and train data"""

train_contexts, train_questions, train_answers = read_input('train-v2.0.json')
valid_contexts, valid_questions, valid_answers = read_input('dev-v2.0.json')

"""Printing 5 random samples to check if data is uploaded and processed correctly"""

ind = random.sample(range(0, len(train_answers)), 5)
for index in ind:
    print('Q: ',train_questions[index],'\n')
    print("Context:\n")
    print(train_contexts[index])
    print(f"\nAnswer:[{train_answers[index]}]\n")
    print("*" * 100)

"""Bert models require the end position of the answers, therefore finding the end point of for each answer"""

def end_point(answers: list, contexts: list) -> list:
    _answers = answers.copy()
    for answer, context in zip(_answers, contexts):
        end_idx = answer['text']
        start_idx = answer['answer_start']
        answer['answer_end'] = start_idx + len(end_idx)
    return _answers

train_answers = end_point(train_answers, train_contexts)
valid_answers = end_point(valid_answers, valid_contexts)

"""Here we fine-tune the pre-trained model, we use ALBERT model"""

tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True)

"""Encoding the data to train the model"""

def encode_data(contexts: list, questions: list, answers: list) -> dict:
    encodings = tokenizer(contexts, questions, truncation=True, padding=True, return_tensors="pt")
    start_pos, end_pos = list(), list()

    for index in range(len(answers)):
        start_value = encodings.char_to_token(index, answers[index]['answer_start'])
        end_value   = encodings.char_to_token(index, answers[index]['answer_end'])
        if start_value is None:
            start_value = tokenizer.model_max_length

        shift = 1
        while end_value is None:
            end_value = encodings.char_to_token(index, answers[index]['answer_end'] - shift)
            shift += 1

        start_pos.append(start_value)
        end_pos.append(end_value)

    encodings.update({
        'start_positions': start_pos, 'end_positions': end_pos
    })
    return encodings

"""We use sample of data to train since training the full dataset will take lot of time and with the technological constraint, It is hard"""

train_encodings = encode_data(train_contexts[0:5000], train_questions[0:5000], train_answers[0:5000])
valid_encodings = encode_data(valid_contexts[0:500], valid_questions[0:500], valid_answers[0:500])

"""Deleting unwanted variables to prevent crashing"""

del train_contexts, train_questions, train_answers
del valid_contexts, valid_questions, valid_answers

import torch
class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings: dict) -> None:
        self.encodings = encodings

    def __getitem__(self, index: int) -> dict:
        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}
    
    def __len__(self):
        return len(self.encodings['input_ids'])

train_ds = SquadDataset(train_encodings)
valid_ds = SquadDataset(valid_encodings)

"""Loading some weights of Pre-Trained model for retraining"""

from transformers import AutoModelForQuestionAnswering
model = AutoModelForQuestionAnswering.from_pretrained('roberta-base')
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
model.train()

"""Training the weights of model using the new data"""

from collections import defaultdict
import torch
from torch.optim.optimizer import Optimizer


class LookaheadWrapper(Optimizer):

    def __init__(self, optimizer, la_steps=5, la_alpha=0.8, pullback_momentum="none"):
        self.optimizer = optimizer
        self._la_step = 0 
        self.la_alpha = la_alpha
        self._total_la_steps = la_steps
        pullback_momentum = pullback_momentum.lower()
        assert pullback_momentum in ["reset", "pullback", "none"]
        self.pullback_momentum = pullback_momentum

        self.state = defaultdict(dict)

        for group in optimizer.param_groups:
            for param in group['params']:
                param_state = self.state[param]
                param_state['cached_params'] = torch.zeros_like(param.data)
                param_state['cached_params'].copy_(param.data)
                if self.pullback_momentum == "pullback":
                    param_state['cached_mom'] = torch.zeros_like(param.data)

    def __getstate__(self):
        return {
            'state': self.state,
            'optimizer': self.optimizer,
            'la_alpha': self.la_alpha,
            '_la_step': self._la_step,
            '_total_la_steps': self._total_la_steps,
            'pullback_momentum': self.pullback_momentum
        }

    def zero_grad(self):
        self.optimizer.zero_grad()

    def get_la_step(self):
        return self._la_step

    def state_dict(self):
        return self.optimizer.state_dict()

    def load_state_dict(self, state_dict):
        self.optimizer.load_state_dict(state_dict)

    def _backup_and_load_cache(self):
        for group in self.optimizer.param_groups:
            for param in group['params']:
                param_state = self.state[param]
                param_state['backup_params'] = torch.zeros_like(param.data)
                param_state['backup_params'].copy_(param.data)
                param.data.copy_(param_state['cached_params'])

    def _clear_and_load_backup(self):
        for group in self.optimizer.param_groups:
            for param in group['params']:
                param_state = self.state[param]
                param.data.copy_(param_state['backup_params'])
                del param_state['backup_params']

    @property
    def param_groups(self):
        return self.optimizer.param_groups

    def step(self, closure=None):
        loss = self.optimizer.step(closure)
        self._la_step += 1

        if self._la_step >= self._total_la_steps:
            self._la_step = 0
    
            for group in self.optimizer.param_groups:
                for param in group['params']:
                    param_state = self.state[param]
                    param.data.mul_(self.la_alpha).add_(param_state['cached_params'], alpha=1.0 - self.la_alpha)  # crucial line
                    param_state['cached_params'].copy_(param.data)
                    if self.pullback_momentum == "pullback":
                        internal_momentum = self.optimizer.state[param]["momentum_buffer"]
                        self.optimizer.state[param]["momentum_buffer"] = internal_momentum.mul_(self.la_alpha).add_(
                            1.0 - self.la_alpha, param_state["cached_mom"])
                        param_state["cached_mom"] = self.optimizer.state[param]["momentum_buffer"]
                    elif self.pullback_momentum == "reset":
                        self.optimizer.state[param]["momentum_buffer"] = torch.zeros_like(param.data)

        return loss

"""Model training"""

# Initialize adam optimizer with weight decay to minimize overfit

from transformers import AdamW

base  = AdamW(model.parameters(), lr=1e-4)
optim = LookaheadWrapper(base)

"""Loading the newly trained weights to the model"""

from torch.utils.data import DataLoader
from tqdm import tqdm

import warnings
warnings.simplefilter("ignore")


# Initialize data loader for training data

train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)


for epoch in range(5):
    model.train()
    loop = tqdm(train_loader, leave=True)
    for batch in loop:
        optim.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        start_positions = batch['start_positions'].to(device)
        end_positions = batch['end_positions'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask,
                        start_positions=start_positions, end_positions=end_positions)
        loss = outputs[0]
        loss.backward()
        optim.step()
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())

"""Saving the model"""

# Saving the model in a local directory

MODEL_DIR = "./model"
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)
tokenizer.save_pretrained(MODEL_DIR)
model.save_pretrained(MODEL_DIR)
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
MODEL_DIR = "./model"
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)
model = AutoModelForQuestionAnswering.from_pretrained(MODEL_DIR)

"""Model Evaluation

Testing model on data and finding EM score and F1 Score
"""

from torch.utils.data import DataLoader
from sklearn.metrics import f1_score
model.eval()
model = model.to(device)

validation_loader = DataLoader(valid_ds, batch_size=16)
accuracy_scores = list()
start_true_all, end_true_all = [], []
start_pred_all, end_pred_all = [], []

for batch in validation_loader:
    with torch.no_grad():
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        start_true = batch['start_positions'].to(device)
        end_true = batch['end_positions'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask)
        start_pred = torch.argmax(outputs['start_logits'], dim=1)
        end_pred = torch.argmax(outputs['end_logits'], dim=1)
        accuracy_scores.append(((start_pred == start_true).sum() / len(start_pred)).item())
        accuracy_scores.append(((end_pred == end_true).sum() / len(end_pred)).item())
        start_true_all.extend(start_true.tolist())
        end_true_all.extend(end_true.tolist())
        start_pred_all.extend(start_pred.tolist())
        end_pred_all.extend(end_pred.tolist())


# Calculate the average accuracy score
average_accuracy = sum(accuracy_scores) / len(accuracy_scores)
print(f"Model's score based on Exact Match: {average_accuracy}")


# Calculating F1 score for start and end positions

start_f1 = f1_score(start_true_all, start_pred_all, average='macro')
end_f1 = f1_score(end_true_all, end_pred_all, average='macro')
overall_f1 = (start_f1 + end_f1) / 2

print(f"F1 score of the model: {overall_f1:.3f}")

"""Function to test the output of the model"""

def get_answers_from_context(input_text: str, input_questions: list) -> list:
    encoded_inputs = tokenizer([input_text]*len(input_questions), input_questions, truncation=True, padding=True, return_tensors="pt")
    encoded_inputs = encoded_inputs.to(device)
    outputs = model(**encoded_inputs)
    start_positions = torch.argmax(outputs['start_logits'], dim=1)
    end_positions = torch.argmax(outputs['end_logits'], dim=1)  
    answers = list()
    for i, (start_index, end_index) in enumerate(zip(start_positions, end_positions)):
        tokens = tokenizer.convert_ids_to_tokens(encoded_inputs['input_ids'][i][start_index:end_index+1])
        answers.append(tokenizer.convert_tokens_to_string(tokens) )
    print("Context:")
    print(input_text)
    print()
    for question, answer in zip(input_questions, answers):
        print(f"Q:  {question}")
        print(f"A:  {answer}")
        print("-"*50)
    return answers

"""Sample answers from the model"""

context = """ BeyoncÃ© Giselle Knowles-Carter (/biËËˆjÉ’nseÉª/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of BeyoncÃ©'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles "Crazy in Love" and "Baby Boy".	
"""
questions = [
    "What was the name of BeyoncÃ©'s first solo album?",
    "who managed the BeyoncÃ©'s group?","Is BeyoncÃ© a actress?"
]

_ = get_answers_from_context(context, questions)

